# 發想

原本在試著使用LangGraph 來做一個詞根詞綴拆分並解析的機器人
但目前的結論上來說算是失敗的，本篇算是一個Try Error的歷程記錄

## Try 1. RAG 資料檢索

本來打算使用RAG來強化自己的資料庫檢索  
使用詞根資料庫當作data來餵給RAG  

我一開始使用512chunk的方式單筆餵入，
但發現效果非常差
即便資料庫中確實含有相關的單詞，
但由於chunk設置太大，導致最相關詞根只占doc中的極小一部分，無法被抓出來

第二次嘗試更改了doc 的讀取設置，  
改採用單行(單一詞根+中文解析)的方式來拆分doc  
但效果仍然不彰，推測是因為將目標單字的片段搜尋也可能會被格式近似的詞搶走關聯性  
RAG無法讓AI理解相關詞綴的關聯  
也可能是單純的詞根資料庫不適合餵給RAG  
需要另外進行處理，總之RAG的方法暫時放棄

## Try. 2 Multi AI Agents

在放棄了使用RAG之後，原本試圖的做法是使用AI Agent拆分細項工作
進而試圖讓LLM理解各個步驟目標為何
主要的嘗試時間花在於調整system instruction  
但效果不彰
原本的設計格式是由第一個AI Agent做第一次的詞根詞綴拆解
拆解完後的每個詞根詞綴部分由第二個AI Agent 思考可不可以繼續拆解  

但結論來看這樣的設計十分不良，當第一個Agent不會拆解而返回了整段文字
第二個Agent也無法理解如何拆解
理論上把詞根、前綴、後綴，這幾個工作進行分工可能會得到比較好的效果  
但這樣的嘗試暫時停止了

我去找了相關的文章，
也嘗試使用了gemini與chatGPT兩者進行詞根詞綴拆解
gemini的表現不如預期，而GPT4.0本身則可以達到類似效果
參考了其他人的文章，發現他是使用GPT做到詞根拆解的
但是費用高得嚇人(可以理解token費用，但感覺就像是拿大槌敲雞蛋)
因此這個設想從成本考量來看也不太符合效益

## Try.3 NLP進行預處理

從前面得出來的發想
現在的LLM其實處理詞根詞綴仍然是有困難的，從成本來看如此，從分工來看也有難度，
但對於每部分的詞根去進行拉丁文詞根查表似乎不難做到
而這也使得這樣子的設計優化似乎有「價值」？
使用NLP的技術進行詞綴拆分，再進拉丁文資料庫用舊有的字典形式查表
由AI Agent進行查詢紀錄的整合與控管
似乎是比較符合設計效益的選擇
但NLP對於文字處理的概念我幾乎忘得差不多  
因此開始有了後續的紀錄研究

## NLP 詞根詞綴拆分


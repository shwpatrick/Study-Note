# 概覽

# DLA是什麼?

AI有大量的運算，所以透過較佳的資料搬運與儲存策略可以減少大量成本  
解決問題 ： 盡可能降低運算時資料搬運的成本  

整體架構可以以此理解分層，越低層級間的交互就可以越少耗能  
所以設計目標就是盡可能在低層級完成運算與傳輸  

| 層級高到低(越低能耗越小)                                                              |                         |
| -------------------------------------------------------------------------- | ----------------------- |
| DRAM                                                                       | 最外圍、最貴                  |
| Global Buffer                                                              | 常用於DRAM資料緩存, 權重、計算中間結果等 |
| PE Array ：由多個PE組成的陣列                                                       | PE與PE間的資料搬運，空間架構影響了對傳距離 |
| PE (processing engine) 包含<br>- ALU (算術邏輯單元)<br>- Register File (本地記憶體)<br> |                         |

關鍵字：  
空間架構(SA)：PE之間可以互傳，所以PE陣列的組成方式很大影響傳輸距離  
數據流(Data flow)：PE與PE間的資料搬運策略  
通常各個平台有各自的數據流最佳化策略  


# CNN 層的運算需求

CNN 每層對於這些運算的實際需求與考量不同  

| 層 | 是否重要           |
| ---- | -------------- |
| CONV | 考量重點           |
| FC   | 考量重點           |
| POOL | 與CONV使用相同的解決方案 |
| ACT  | 可忽略            |


# 實際資料流的策略設計考量

- psum 計算的中間產物，可以暫不處理，但會帶來記憶體空間的壓力
- 重用性(Reuse)：許多架構可以重複使用，不需要每次重新讀取，減緩高帶寬需求和高能源消耗
	- convolutional reuse : CONV 層獨有
	- filter reuse : CONV, FC 層都有
	- ifmap reuse: CONV, FC 層都有
- Mapping: 每層的大小不同，所以需要考慮到實際運算元跟層的映射關係

# CNN 與 ISP 的不同

雖然ISP也有考慮卷積優化，但不能照搬給CNN，因為  

- CNN CONV 是 4D (ISP 是 2D)  
- CNN的Filter 不固定 (訓練計算得來)
- 
